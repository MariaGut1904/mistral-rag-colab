# -*- coding: utf-8 -*-
"""Final RAG Pipeline

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RKjFxByL43Mpl84LRZULkIS9pofJ-aun
"""

# Install required packages
!pip install -q torch llama-cpp-python==0.2.90 llama-index pymupdf llama-index-llms-llama-cpp llama-index-embeddings-huggingface sentence-transformers --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu123

import torch
import fitz
import os
from llama_index.core import VectorStoreIndex, Document, Settings
from llama_index.core.retrievers import VectorIndexRetriever
from llama_index.core.query_engine import RetrieverQueryEngine
from llama_index.llms.llama_cpp import LlamaCPP
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.core.node_parser import SentenceSplitter
from llama_index.core.postprocessor import SentenceTransformerRerank
from google.colab import files

# Verify CUDA availability
print(f"CUDA available: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"GPU: {torch.cuda.get_device_name(0)}")

# Download Mistral GGUF model if not present
MODEL_PATH = "/content/mistral-7b-instruct-v0.2.Q4_K_M.gguf"
if not os.path.exists(MODEL_PATH):
    !wget https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/resolve/main/mistral-7b-instruct-v0.2.Q4_K_M.gguf -O {MODEL_PATH}

# Initialize LLM with GPU acceleration
Settings.llm = LlamaCPP(
    model_path=MODEL_PATH,
    temperature=0.7,
    max_new_tokens=512,
    context_window=2048,
    model_kwargs={
        "n_gpu_layers": -1,  # Use all GPU layers
        "n_ctx": 2048,
        "verbose": False
    }
)

# Initialize embedding model
Settings.embed_model = HuggingFaceEmbedding(
    model_name="BAAI/bge-small-en-v1.5"
)

# Configure text splitter for better chunking
text_splitter = SentenceSplitter(
    chunk_size=512,
    chunk_overlap=128,
    separator="\n"
)

def process_pdfs(pdf_paths):
    """Process multiple PDFs and return combined nodes with metadata"""
    all_nodes = []
    total_words = 0

    for pdf_path in pdf_paths:
        doc = fitz.open(pdf_path)
        text = "\n".join([page.get_text() for page in doc])
        word_count = len(text.split())
        total_words += word_count
        print(f" Extracted {word_count} words from {os.path.basename(pdf_path)}")

        # Create nodes with  chunking
        documents = [Document(text=text, metadata={"source": pdf_path})]
        nodes = text_splitter.get_nodes_from_documents(documents)
        all_nodes.extend(nodes)

    print(f"\n Processed {len(pdf_paths)} PDFs ({total_words} total words)")
    return all_nodes

def build_rag_pipeline(nodes):

    # Create index
    index = VectorStoreIndex(nodes)

    # Configure retriever
    retriever = VectorIndexRetriever(
        index=index,
        similarity_top_k=3,  # Increased for multi-doc search
    )

    # Add reranker for better results
    reranker = SentenceTransformerRerank(
        model="cross-encoder/ms-marco-MiniLM-L-6-v2",
        top_n=5  # Increased for multi-doc search
    )

    # Assemble query engine
    query_engine = RetrieverQueryEngine(
        retriever=retriever,
        node_postprocessors=[reranker]
    )
    return query_engine

def upload_pdfs():

    uploaded = files.upload()
    return list(uploaded.keys())

# Main execution
def main():
    # Upload PDFs
    print("Upload multiple PDF files (Ctrl+Click to select multiple)")
    pdf_paths = upload_pdfs()

    if not pdf_paths:
        print(" No PDFs uploaded. Exiting.")
        return

    # Process PDFs and build pipeline
    nodes = process_pdfs(pdf_paths)
    query_engine = build_rag_pipeline(nodes)

    # Test queries
    test_queries = [
        "What are the late payment penalties?",
        "Summarize the key terms of this contract",
        "List all financial obligations mentioned"
    ]

    for query in test_queries:
        print(f"\n{'='*50}\nQuery: {query}\n{'='*50}")
        response = query_engine.query(query)
        print(f"\nResponse: {response}\n{'='*50}")

if __name__ == "__main__":
    main()